# ----------------- Model and Data Paths -----------------
model_name_or_path: "/hai/scratch/fangwu97/xu/SimPO_slurm/outputs/gemma-2-9b-it"
dataset_path: "/hai/scratch/fangwu97/xu/SimPO_slurm/data/gemma2_ufb_part1.jsonl"
eval_dataset_path: "/hai/scratch/fangwu97/xu/SimPO_slurm/data/gemma2_ufb_part1_test.jsonl"
output_dir: "./outputs/gemma-2-9b-it-dpo-iter1" # Directory to save the final model

# ----------------- Wandb Logging -----------------
wandb_project_name: "gemma2-9b-dpo-project" # Your project name on Weights & Biases
wandb_run_name: "dpo-run-iter1"         # A name for this specific run

# ----------------- DPO Parameters -----------------
beta: 0.1 # The beta parameter for DPO loss

# ----------------- Training Hyperparameters -----------------
learning_rate: 5.0e-7
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 16
num_train_epochs: 1
max_length: 2048
max_prompt_length: 1800
bf16: true # Use bfloat16 precision
seed: 42  # <-- 新增: 用于保证实验的可复现性
warmup_ratio: 0.1 # <-- 新增: 3% 的训练步数用于学习率预热
lr_scheduler_type: "cosine" # <-- 新增: 学习率调度器类型, 'cosine' 或 'linear' 都是很好的选择


# ----------------- Evaluation and Saving -----------------
evaluation_strategy: "steps"
eval_steps: 15
save_strategy: "steps"
save_total_limit: 1           # 只保留 1 个 checkpoint
load_best_model_at_end: true  # 训练结束时加载最优模型
metric_for_best_model: "eval_loss"